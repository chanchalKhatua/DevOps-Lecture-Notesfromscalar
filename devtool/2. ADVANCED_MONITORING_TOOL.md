# Advanced Monitoring Techniques with Prometheus

## Agenda

- Prometheus Storage
- Alertmanager
- Configuring Alerts using Slack
- Push Gateway

---

## Prometheus Storage

### Types of Storage

- **Local Storage (TSDB)**

---

## ðŸ“Š 1. **Prometheus Metric Sample**
```text
http_requests_total{method="GET", status="200"} => 250 at 2025-04-17T12:00:00Z
```

### ðŸ” Explanation:
- **`http_requests_total`**: A **counter** metric that increases over time, tracking total HTTP requests.
- **Labels**: Key-value pairs used to differentiate this metric:
  - `method="GET"` â†’ the HTTP method.
  - `status="200"` â†’ the HTTP response status code.
- **Value**: `250` means Prometheus has recorded 250 successful GET requests so far.
- **Timestamp**: When this value was recorded â†’ `2025-04-17T12:00:00Z`.

This is a basic but powerful concept: **metrics + labels** let you filter and aggregate data for dashboards, alerts, and capacity planning.

---

## ðŸ—ï¸ 2. **Prometheus Storage System**

Prometheus stores time series data locally using a custom **TSDB (Time Series Database)**.

### ðŸ—‚ï¸ Directory Structure Breakdown:

```
/prometheus
â”œâ”€â”€ WAL/                   # Write-Ahead Log
â”œâ”€â”€ chunks_head/           # Head Block (in-memory data not compacted yet)
â”œâ”€â”€ 01FJ.../               # Compacted block (2-hour segments)
â”‚   â”œâ”€â”€ chunks/            # Compressed chunks of data
â”‚   â”œâ”€â”€ index              # Label index for querying
â”‚   â””â”€â”€ meta.json          # Metadata (block start/end time)
â””â”€â”€ lock                   # Ensures only one Prometheus process accesses the data
```

### ðŸ§  Components Explained:

#### âœ… WAL/ (Write-Ahead Log)
- Temporary storage for new data **before** itâ€™s saved into blocks.
- Ensures **data recovery** in case of crashes.
- Rolled into the head block periodically.

#### âœ… chunks_head/
- **Recent in-memory data**.
- Represents **active time series**.
- Itâ€™s not yet compacted; compacted blocks only happen every **2 hours**.
- **If Prometheus is restarted, WAL + this gets replayed to restore state**.

#### âœ… Compacted Block Directory (e.g., `01FJ...`)
- Created after **compaction** (typically 2 hours of data).
- Each contains:
  - `chunks/` â†’ actual compressed samples
  - `index` â†’ fast lookup for labels & series
  - `meta.json` â†’ start and end timestamp of this block, and metadata

---

## â±ï¸ 3. **Retention Settings**

Set using Prometheus **startup flags** (not in `prometheus.yml`):

```yaml
command:
  - "--storage.tsdb.retention.time=45d"
  - "--storage.tsdb.retention.size=30GB"
```

### ðŸ’¡ Details:

#### `--storage.tsdb.retention.time=45d`
- Keep time series data for the **last 45 days**.
- Older data is **deleted automatically** to save space.

#### `--storage.tsdb.retention.size=30GB`
- Prometheus deletes **oldest blocks** if disk usage exceeds 30GB.
- Useful to prevent disk full issues.

You can set **both together**: Prometheus deletes data **when either** limit is reached.

---

## ðŸ³ 4. **Docker Compose Setup**

```yaml
prometheus:
  image: prom/prometheus:latest
  container_name: prometheus
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus
  ports:
    - "9090:9090"
  networks:
    - monitoring
  command:
    - "--config.file=/etc/prometheus/prometheus.yml"
    - "--storage.tsdb.retention.time=45d"
    - "--storage.tsdb.retention.size=30GB"
```

### ðŸ§¾ Details:

- **Volumes**:
  - `./prometheus.yml:/etc/prometheus/prometheus.yml`: Custom scrape config.
  - `prometheus_data:/prometheus`: Persist metrics between container restarts.

- **Ports**: 9090 is the Prometheus web UI.

- **Networks**: Use `monitoring` network for integration with Grafana, exporters, etc.

- **Command**: Overrides default settings to include:
  - Config file
  - Retention time (45 days)
  - Retention size (30GB)

---

## ðŸ§  5. **What prometheus.yml Is For**

The `prometheus.yml` config file is used **only for scrape-related settings**:

### ðŸŽ¯ Responsibilities:

- Scrape targets (e.g., Node Exporter, cAdvisor)
- Scrape interval (default: 15s)
- Job labels
- Relabeling rules
- External service discovery (Kubernetes, Consul, etc.)
- Recording & alerting rules
- Remote write / read

### âŒ What it does *not* include:
- **Storage configuration**
- **Retention settings**
- **Disk usage limits**

Those are **command-line flags** or Docker args, not YAML configs.

---

## ðŸ”š Summary Table

| Concept                      | Purpose                                                                 |
|-----------------------------|-------------------------------------------------------------------------|
| `http_requests_total`       | Counter metric with labels for method/status                           |
| `WAL/`                      | Stores recent data before compaction (for crash recovery)              |
| `chunks_head/`              | In-memory data not yet in blocks                                        |
| `01FJ.../` blocks            | Compacted, queryable data chunks                                       |
| `prometheus.yml`            | Configures scraping, targets, and alerts                                |
| `--storage.tsdb.retention.*`| Controls how long & how much data is kept                              |
| Docker `volumes:`           | Persists config & data on disk                                         |

---
---

# ðŸ§  Prometheus Write-Ahead Logs (WAL) and Chunks Head: In-Depth

---

## ðŸŸ¡ 1. What is a Write-Ahead Log (WAL)?

A **Write-Ahead Log** is a file where changes are **written first** before they are applied to the actual database.
1. **Buffer incoming data** in memory.
2. **Persist data on disk quickly**, before itâ€™s compacted and stored in long-term blocks.
3. **Recover data** in case Prometheus crashes or is shut down ungracefully
   
- **Purpose:** Ensures **durability** and **recovery** in case Prometheus crashes or restarts.
- **What it stores:** Every time a new sample is ingested, Prometheus **immediately writes it to the WAL** before doing
anything else.
- **Stored files:** Named sequentially (e.g. `00000014`, `00000015`, etc.), plus optional checkpoints

### ðŸ” Key Points:

| Feature | Explanation |
|--------|-------------|
| **Definition** | A file that stores incoming metric data **before** it is processed and saved permanently. |
| **Location** | `/prometheus/wal/` |
| **File Names** | Sequential: `00000001`, `00000002`, ..., plus optional `checkpoint.XXXX` folders |
| **Purpose** | Acts as a **buffered write log** for reliability and recovery |

---

## ðŸ” 2. Why WAL is Important

| Goal | How WAL Helps |
|------|---------------|
| **Data durability** | Prevents data loss during unexpected crashes |
| **Crash recovery** | Replays logs on restart to rebuild in-memory chunks |
| **High performance** | Buffers writes efficiently before compaction |

---

## ðŸ§­ 3. WAL Workflow â€“ Step-by-Step

### ðŸ”‚ Data Flow:
1. ðŸŸ¢ Scrape: Prometheus scrapes metrics from targets.
2. ðŸ“ **Write to WAL**: The sample is immediately written to a WAL segment.
3. ðŸ§  **Store in memory (Chunks Head)** for fast querying.
4. ðŸ“¦ Every 2 hours, data is **compacted into TSDB blocks**.
5. ðŸ§¹ Old WAL segments are deleted after checkpointing.
---
![image](https://github.com/user-attachments/assets/0b8f6eb0-ea30-4adc-858c-bd6452d40067)

---

## ðŸ“„ 4. WAL File Types

| Type | Description |
|------|-------------|
| `00000001`, `00000002`, etc. | Actual WAL segment files with ingested samples |
| `checkpoint.XXXX` folders | Snapshots used to **speed up recovery** |
| Metadata files | Track segment start/end positions, transaction markers |

---

## ðŸ§  5. Chunks Head â€” Active In-Memory Buffer

**Chunks Head** is where Prometheus stores **active time series data** before it's flushed into a permanent block.

- **Purpose:** This holds the actual **in-memory time series chunks** before they are flushed into persistent block
storage (~2-hour blocks).
- **What it stores:** Time series data points that are **actively being collected and not yet compacted**.
- **When itâ€™s used:** These chunks are **not flushed to disk** until a block is created. Until then, they live in memory and
are backed by WAL.

### ðŸ§¾ Details:

| Attribute | Description |
|-----------|-------------|
| **Path** | `/prometheus/chunks_head/` |
| **Purpose** | Holds in-memory chunks for fast updates & reads |
| **Backed by** | WAL (for durability) |
| **Compacted every** | ~2 hours (default) |
| **Stored data** | Raw time series samples (`timestamp`, `value`, `labels`) |

---

## ðŸ” 6. Relationship Between WAL & Chunks Head

| Step | WAL | Chunks Head |
|------|-----|--------------|
| Initial write | âœ… (on disk) | âœ… (in memory) |
| During active collection | Used for recovery | Used for fast querying |
| After compaction | May be checkpointed & deleted | Flushed into block |
| On restart | WAL is replayed | Chunks Head is reconstructed |

---

## ðŸ§± 7. TSDB Block Structure (Created from Chunks Head)

Every 2 hours, data in Chunks Head is compacted into a block directory:

```
/prometheus/01FJXXXX/
â”œâ”€â”€ chunks/      # Compressed time series data
â”œâ”€â”€ index        # Label/series index
â””â”€â”€ meta.json    # Metadata (timestamps, source info)
```

---

## ðŸ—ƒï¸ 8. WAL Lifecycle in Summary

```plaintext
Scrape â†’ WAL (disk) â†’ Chunks Head (memory) â†’ TSDB Block (permanent)
```

1. **Scraped** sample is written to WAL and memory.
2. WAL persists data even if Prometheus crashes.
3. Chunks Head builds compressed chunks.
4. Compactor creates a TSDB block every 2h.
5. WAL segments are **checkpointed and deleted** post-block creation.

---

## ðŸ”§ 9. How to Configure Retention (Optional but Important)

These are not in `prometheus.yml`, but passed as flags in Docker or systemd:

```yaml
command:
  - "--storage.tsdb.retention.time=45d"
  - "--storage.tsdb.retention.size=30GB"
```

| Flag | Purpose |
|------|---------|
| `--storage.tsdb.retention.time` | Retain time series for 45 days |
| `--storage.tsdb.retention.size` | Cap storage usage at 30GB |

---

## ðŸ“Œ 10. Key Facts Summary Table

| Concept          | Description |
|------------------|-------------|
| WAL              | Fast disk write log for durability |
| Chunks Head      | In-memory buffer of recent time series |
| Checkpoint       | Optimized snapshot of WAL |
| Compaction       | Happens ~every 2h, flushes memory to disk |
| Block Directory  | Contains `chunks`, `index`, and `meta.json` |
| WAL Files        | Sequentially numbered, deleted post-compaction |
| Recovery         | WAL replayed to rebuild memory after crash |

---

### Remote Storage
---

# ðŸ“¦ Prometheus Remote Storage and Best Practices

## Remote Storage Integration
For **long-term durability** and **centralized querying**, Prometheus can **integrate with remote storage systems**.

Prometheus supports two types of remote integrations:

- **Remote Write**: Push data to external systems  
  (e.g., **Thanos**, **Cortex**, **VictoriaMetrics**, **InfluxDB**)

- **Remote Read**: Query remote data back into Prometheus UI / via PromQL.

### Example Configuration:
```yaml
remote_write:
  - url: "http://thanos-receive:10902/api/v1/receive"

remote_read:
  - url: "http://thanos-query:9090"
```

---

# âš™ï¸ Thanos Integration (for Centralized, Scalable Storage)
- **Thanos Receive** will accept remote writes.
- **Thanos Query** will serve remote reads, allowing you to query long-term and short-term data seamlessly.

---

# ðŸ–¥ï¸ Node Exporter Metrics
- **Exporter Port**: `9100`
- Used to collect **node (server) metrics** like CPU, memory, disk usage, etc.

---

# ðŸ›¡ï¸ Best Practices for Prometheus Storage

1. **Monitor Disk Space**  
   - Always monitor your Prometheus server's storage.
   - Alert if usage crosses critical thresholds.

2. **Optimize Retention Periods**  
   - Set Prometheus' `--storage.tsdb.retention.time` wisely based on your needs (e.g., 15 days, 30 days).
   
3. **Use External Storage**  
   - For long-term retention and scalability, integrate external systems like **Thanos**, **Cortex**, etc.

4. **Implement Sharding**  
   - In large-scale environments, deploy **multiple Prometheus instances**.
   - Use a load balancer or scrape sharding to distribute the ingestion load.

5. **Regular Cleanup**  
   - Configure automatic cleanup of old data.
   - Ensure blocks or WAL files older than the retention period are deleted automatically.

---
---

## Alertmanager
### What are alerts?
 - Resource exhaustion (e.g., high CPU or memory usage).
 - Failures (e.g., a service going down).
 - Threshold breaches (e.g., response time exceeding acceptable limits).
### When to trigger alerts
 1. Performance Issues:
     - High CPU or memory usage, slow database queries, or application response times out of range.
 2. Failures:
     - Service crashes, database outages, or network failures.
 3. Security Events:
     - Unauthorized access attempts or unusual traffic spikes.
 4. Business Metrics:
     - Revenue drops, user signups, or payment failures.
### Why Alerts Are Needed
1. Proactive Monitoring:
    - Detect issues before they affect users or customers.
    - Alerts help anticipate issues before they escalate into major problems, improving system reliability and user
     experience.
2. Prevent Downtime:
    - Immediate alerts allow teams to respond before small issues become major outages.
3. Compliance and SLA Management:
    - Ensure systems comply with service level agreements (SLAs) by notifying when performance drops below agreed
      levels.
 4. Automation and Speed:
    - Alerts automate the process of notifying your team about issues.
    - Without alerts, issues might go unnoticed, leading to prolonged downtime or degraded performance.
5. Quick Response to Critical Issues:
    - Alerts provide real-time visibility into problems, allowing teams to respond quickly and prevent cascading
      failures.
6. Avoid Alert Fatigue:
    - Properly configured alerts highlight important issues while filtering out noise.
7. Improve Decision-Making:
    - Alerts provide insights that can inform decisions, such as prioritizing resources to address a high number of failed
      requests or a network bottleneck.
### Best Practices for alerting
 1. Set Appropriate Thresholds:
   - Avoid setting thresholds too low or too high.
 2. Avoid Alert Fatigue:
   - Configure alerts thoughtfully to reduce noise.
 3. Use Multiple Notification Channels:
   - Ensure alerts reach the right people through email, Slack, SMS, etc.
 4. Regularly Review Alerting Policies:
   - Periodically review and update alerting rules to ensure they remain relevant.
### Alert Types

- Threshold Alerts
- State Change Alerts
- Anomaly Alerts
- Business Metrics

### Features

1. Ingestion: Receives alerts from Prometheus or other clients.
2. Deduplication: Prevents duplicate notifications for the same issue.
3. Grouping: Groups related alerts together to avoid overwhelming your team.
4. Inhibition: Suppresses alerts based on predefined rules (e.g., suppressing low-priority alerts when a high-priority alert
is active).
5. Silencing: Temporarily suppresses alerts during maintenance or known issues.
6. Routing: Sends alerts to different receivers (e.g., email, Slack) based on rules.

In summary Alertmanager is responsible for
- **Receiving alerts** from Prometheus
- **Deduplicating** them
- **Grouping** related alerts
- **Routing** them to the correct notification channels (like Email, Slack, PagerDuty)
- **Silencing** or **inhibiting** alerts when needed (to avoid alert fatigue)

### Workflow

1. Prometheus triggers alert
2. Alertmanager applies rules
3. Alertmanager sends notification
### How does Alert manager work??
1. Prometheus Triggers an Alert:
 - Based on a condition, such as CPU Usage > 90%.
2. Alertmanager Processes the Alert:
 - It applies rules for grouping, silencing, or inhibition.
3. Alertmanager Sends Notifications:
 - Alerts are routed to the appropriate receivers, such as email, Slack, or PagerDuty.

 1. Alerting Rules in Prometheus: Define when alerts should be triggered.
 2. Alertmanager Configuration: Defines how alerts are handled and routed.
### Alert Rule Example (`alerts.yml`)

```yaml
groups:
  - name: example-alerts
    rules:
      - alert: HighCPUUsage
        expr: avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) > 0.85
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 85% for 2 minutes on {{ $labels.instance }}"

```

### Configure Prometheus to Use Alert Rules

```yaml
global:
  scrape_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']

scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']

```

### Alertmanager Email Example

Awesome â€” you've shared a basic **Alertmanager email notification configuration**!  
Let me quickly organize and slightly format it so itâ€™s clean and easy to use:

---

```yaml
global:
  resolve_timeout: 5m

route:
  receiver: 'email-team'

receivers:
  - name: 'email-team'
    email_configs:
      - to: 'devops-team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_identity: 'alertmanager@example.com'
        auth_password: 'your_smtp_password'
```

---

# âœ¨ Key Points:
- **global.resolve_timeout**:  
  If an alert resolves, Alertmanager will wait 5 minutes before considering it resolved (allows for flapping alerts).
  
- **route.receiver**:  
  The default receiver for alerts is `email-team`.

- **receivers.email_configs**:
  - **to**: Where the alert emails are sent (`devops-team@example.com`).
  - **from**: The sender email address.
  - **smarthost**: SMTP server with port (typically 587 for TLS).
  - **auth_username / auth_identity / auth_password**: SMTP authentication credentials.

---
### **"End-to-End Flow"** 
---

ðŸ”¹ **Node Exporter**  
â†’ Exposes system metrics (CPU, memory, disk, etc.)

ðŸ”¹ **Prometheus**  
â†’ Pulls/scrapes metrics from Node Exporter  
â†’ Evaluates alert rules in `alert_rules.yml`

ðŸ”¹ **If alert triggered**  
â†’ Prometheus fires the alert  
â†’ Sends it to **Alertmanager**

ðŸ”¹ **Alertmanager**  
â†’ Manages alerts (grouping, silencing, routing)  
â†’ Sends notifications via Email, Slack, PagerDuty, etc.

---
flowchart TD
    A[Node Exporter] --> B[Prometheus Scrapes Metrics]
    B --> C{Evaluate Alert Rules}
    C -- No Alert --> B
    C -- Alert Triggered --> D[Send Alert to Alertmanager]
    D --> E{Route & Manage Alerts}
    E --> F[Send Notification (Email / Slack / etc.)]

### **The `prometheus.yml`, `docker-compose.yml`, `alertmanager.yml`, and `alerts.yml` files are all integral components in setting up a Prometheus monitoring stack. However, each of these files serves a distinct purpose. Hereâ€™s how they differ:**

---

### 1. **`prometheus.yml`** â€“ **Prometheus Configuration File**

#### Purpose:
- The **`prometheus.yml`** file is the **main configuration file** for Prometheus. It defines how Prometheus scrapes metrics, the targets to monitor, how to handle alerts, and which files contain alerting rules.

#### Key Responsibilities:
- **Scrape Targets**: Specifies which services Prometheus should monitor (e.g., Node Exporter, application exporters) and how often to scrape their metrics.
- **Alerting Setup**: Defines where Prometheus should send alerts (via Alertmanager) when certain conditions are met.
- **Alerting Rules**: Points to external rule files (like `alerts.yml`) for defining the logic that triggers alerts based on metrics.

#### Example:
```yaml
global:
  scrape_interval: 15s  # Defines the default scrape interval

scrape_configs:
  - job_name: "node_exporter"
    static_configs:
      - targets: ["node_exporter:9100"]

alerting:
  alertmanagers:
    - static_configs:
        - targets: ["alertmanager:9093"]

rule_files:
  - alerts.yml  # Specifies the file containing alerting rules
```

### 2. **`docker-compose.yml`** â€“ **Defines Services in Docker**

#### Purpose:
- The **`docker-compose.yml`** file is used to define and configure all the services that are part of the **monitoring stack** (or any Dockerized application). This file specifies how Prometheus, Alertmanager, and other components should run in Docker containers.

#### Key Responsibilities:
- **Define Containers**: Specifies the Docker images to use for services like Prometheus, Alertmanager, and Node Exporter.
- **Networking**: Defines the network configurations (e.g., which ports are exposed and how services can communicate with each other).
- **Volumes**: Mounts configuration files and persistent volumes for services like Prometheus and Alertmanager.

#### Example:
```yaml
version: "3.8"
services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml

  node_exporter:
    image: prom/node-exporter:latest
    container_name: node_exporter
    ports:
      - "9100:9100"

volumes:
  prometheus_data:  # Persistent volume for Prometheus data
```

#### Difference from `prometheus.yml`:
- The **`docker-compose.yml`** file configures the Docker container setup, while **`prometheus.yml`** configures Prometheus itself, including its monitoring targets and alerting rules.
- `docker-compose.yml` is for defining **how** services like Prometheus and Alertmanager run in containers, whereas **`prometheus.yml`** configures **what** Prometheus monitors and how it interacts with Alertmanager.

---

### 3. **`alertmanager.yml`** â€“ **Alertmanager Configuration File**

#### Purpose:
- The **`alertmanager.yml`** file is the configuration file for **Alertmanager**, which handles the alert notifications triggered by Prometheus. It defines how and where to route alerts (e.g., email, Slack).

#### Key Responsibilities:
- **Alert Routing**: Defines where alerts should be sent (e.g., email, Slack, etc.) and how they are grouped.
- **Notification Channels**: Configures notification channels (e.g., Slack Webhook URLs) for sending alerts.
- **Alert Management**: Manages alert silencing, inhibition, and other alert processing rules.

#### Example:
```yaml
global:
  resolve_timeout: 5m  # Time to wait before resolving an alert

route:
  group_by: ['alertname']
  receiver: 'slack-notifications'

receivers:
  - name: 'slack-notifications'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXXX'
        channel: '#alerts'
```

#### Difference from `prometheus.yml`:
- **`alertmanager.yml`** configures **Alertmanager** and defines how it handles and routes alerts. In contrast, **`prometheus.yml`** defines **how Prometheus generates alerts** and the targets it scrapes for data.
- While **`prometheus.yml`** specifies when to trigger alerts based on scraped metrics, **`alertmanager.yml`** defines **what to do** with those alerts once they are triggered (e.g., sending them to Slack).

---

### 4. **`alerts.yml`** â€“ **Prometheus Alerting Rules**

#### Purpose:
- The **`alerts.yml`** file contains the **alerting rules** for Prometheus. It defines **when to trigger an alert** based on the values of metrics scraped by Prometheus. 

#### Key Responsibilities:
- **Define Alert Conditions**: Specifies Prometheus alerting rules using PromQL expressions. These rules evaluate conditions such as high CPU usage, low disk space, or application errors.
- **Alert Severity**: Defines the severity of the alerts (e.g., critical, warning).
- **Alert Annotations**: Adds additional information to alerts (e.g., message summaries, detailed explanations).

#### Example:
```yaml
groups:
  - name: example-alerts
    rules:
      - alert: HighCPUUsage
        expr: avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance) < 0.2
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CPU usage is high on instance {{ $labels.instance }}"
```

#### Difference from `prometheus.yml`:
- **`alerts.yml`** contains the **alerting rules** that define when an alert should fire based on metrics. These rules are referenced in **`prometheus.yml`** (via `rule_files`) to enable Prometheus to evaluate and trigger alerts.
- **`prometheus.yml`** configures how Prometheus scrapes data, where to send alerts, and which rule files to load, while **`alerts.yml`** specifies the actual conditions (in PromQL) under which Prometheus should trigger an alert.

---

### Summary of Differences:

| File                 | Purpose                                                          | Configures                                           |
|----------------------|------------------------------------------------------------------|-----------------------------------------------------|
| **`prometheus.yml`**  | Configures Prometheus itself: scrape targets, alerting, etc.     | Prometheus scraping intervals, alerting, rules files|
| **`docker-compose.yml`** | Defines how services run in Docker containers                  | Configures Prometheus, Alertmanager, Node Exporter services |
| **`alertmanager.yml`** | Configures how Alertmanager routes and processes alerts          | Alertmanager routing, notification channels        |
| **`alerts.yml`**      | Defines alerting rules for Prometheus to evaluate and trigger    | Prometheus alert conditions and severity labels     |

---

Each of these files plays a crucial role in setting up a working monitoring and alerting system with Prometheus and Alertmanager. While **`prometheus.yml`** defines the monitoring aspect (scraping and alerting), **`alertmanager.yml`** and **`alerts.yml`** focus on how to manage and handle alerts once they are triggered.

---
# DEMO
## Configuring Alerts Using Slack
   1. Create alert rules that trigger based on fake threshold values.
   2. Configure Prometheus to send alerts to Alertmanager.
   3. Configure Alertmanager so that it routes alerts to Slack.
   4. Run node exporter to simulate real-time metrics data.
 ---
   - user: Time spent executing user-level processes (your applications, scripts).
   - system: Time spent executing kernel-level operations (OS-level tasks).
   - idle: Time the CPU was not being used
   - iowait: Time spent waiting for I/O operations (disk/network).
   - steal: Time taken by other virtual machines (on shared hosts).
   - nice: Time spent on low-priority processes.
   - irq/softirq: Time handling hardware/software interrupts.

---
### Steps
  #### Step 1: Create a Slack Channel
     1. Open your Slack workspace.
     2. Click **Channels â†’ Create a Channel**.
     3. Name it (e.g., `#temp-devops-assignment`).
     4. Choose public/private, then click **Create**.
#### Step 2: Create an Incoming Webhook
     1. Visit https://api.slack.com/apps
     2. Click **Create New App** â†’ **From Scratch**.
     3. Enter a name and select your workspace.
     4. Go to **Incoming Webhooks** â†’ Toggle it **on**.
     5. Click **â€œAdd New Webhook to Workspaceâ€**.
     6. Choose the channel you created â†’ Click **Allow**.
     7. Copy the generated **Webhook URL** (e.g., `https://hooks.slack.com/services/TXXXX/BXXXX/XXXXXXXX`).
     8. Replace the placeholder in `alertmanager.yml` with this URL:


 updated `docker-compose.yml` file with detailed explanations for each section:

```yaml
version: "3.8"
services:
  # Prometheus Service
  prometheus:
    image: prom/prometheus:latest  # Use the latest Prometheus image from Docker Hub
    container_name: prometheus  # Name the container 'prometheus'
    ports:
      - "9090:9090"  # Expose Prometheus UI on port 9090
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml  # Mount the custom Prometheus config
      - ./alerts.yml:/etc/prometheus/alerts.yml  # Mount the custom alerting rules
      - prometheus_data:/prometheus  # Mount the named volume for Prometheus data storage
    command:
      - --config.file=/etc/prometheus/prometheus.yml  # Specify the Prometheus config file
      - --storage.tsdb.path=/prometheus  # Specify where Prometheus stores its time-series data

  # Alertmanager Service
  alertmanager:
    image: prom/alertmanager:latest  # Use the latest Alertmanager image from Docker Hub
    container_name: alertmanager  # Name the container 'alertmanager'
    ports:
      - "9093:9093"  # Expose Alertmanager UI on port 9093
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml  # Mount the custom Alertmanager config
    command:
      - --config.file=/etc/alertmanager/alertmanager.yml  # Specify the Alertmanager config file

  # Node Exporter Service
  node_exporter:
    image: prom/node-exporter:latest  # Use the latest Node Exporter image from Docker Hub
    container_name: node_exporter  # Name the container 'node_exporter'
    ports:
      - "9100:9100"  # Expose Node Exporter metrics on port 9100
    # No additional volumes are necessary for the Node Exporter since it collects system metrics directly from the host

# Define named volumes for data persistence
volumes:
  prometheus_data:  # This volume will store Prometheus time-series database (TSDB) data persistently
```

### Explanation of the Key Sections:

#### `prometheus` service:
- **image**: The container will use the latest `prom/prometheus` image.
- **container_name**: The container will be named `prometheus`.
- **ports**: The container's internal port `9090` (default Prometheus port) will be mapped to the host machine's port `9090` for web access.
- **volumes**: This section mounts files from the local filesystem to the container:
  - `./prometheus.yml` to `/etc/prometheus/prometheus.yml`: This file contains the configuration for scraping targets.
  - `./alerts.yml` to `/etc/prometheus/alerts.yml`: This file defines the alerting rules for Prometheus.
  - `prometheus_data:/prometheus`: The persistent volume to store Prometheus data (e.g., metrics and time-series data).
- **command**: The command specifies the configuration file and data storage path for Prometheus.

#### `alertmanager` service:
- **image**: Uses the latest `prom/alertmanager` image.
- **container_name**: The container will be named `alertmanager`.
- **ports**: Exposes Alertmanagerâ€™s UI on port `9093` for access to alerting settings and status.
- **volumes**: Mounts the `alertmanager.yml` configuration file, where you can configure how alerts are handled, such as routing, silencing, and notification channels (e.g., Slack).
- **command**: Specifies the path to the Alertmanager configuration file.

#### `node_exporter` service:
- **image**: Uses the latest `prom/node-exporter` image.
- **container_name**: The container will be named `node_exporter`.
- **ports**: Exposes port `9100`, which is the default port for Node Exporter to expose system metrics.
- **no volumes**: Node Exporter does not need additional files or configurations to collect system metrics, so there is no volume mounting here.

#### `volumes`:
- **prometheus_data**: A named volume that will store Prometheus' time-series data (TSDB) persistently. This ensures that even if the Prometheus container is stopped or removed, the data is preserved.

### Running the Stack:

To start the services defined in `docker-compose.yml`, run the following command in the same directory:

```bash
docker-compose up -d
```

This will pull the required images and start Prometheus, Alertmanager, and Node Exporter in detached mode (background). You can check the status of the containers with:

```bash
docker-compose ps
```

### Additional Notes:
- **Prometheus Configuration (`prometheus.yml`)**: This file is essential for defining your scrape targets (e.g., which metrics to collect, how frequently, etc.).
- **Alerting Configuration (`alerts.yml`)**: This file defines the alerting rules, such as when an alert should be fired based on certain conditions (e.g., high CPU usage).
- **Alertmanager Configuration (`alertmanager.yml`)**: This file configures how alerts should be routed (e.g., sending notifications to Slack, email, etc.).

The line:

```yaml
command:
  - --config.file=/etc/alertmanager/alertmanager.yml  # Specify the Alertmanager config file
```

is specifying a custom configuration file for **Alertmanager** when the container is started. Here's why it's necessary:

### Purpose of the `command` Directive:
- The `command` directive in `docker-compose.yml` allows you to override the default command that the container runs when it's started. By default, the Alertmanager container might run with some default settings, but in most cases, you want to provide a custom configuration to control how it behaves (e.g., how alerts are routed, how notifications are sent, etc.).

### What Does `--config.file=/etc/alertmanager/alertmanager.yml` Do?
- `--config.file=/etc/alertmanager/alertmanager.yml` is a command-line flag passed to the Alertmanager container to tell it which configuration file to use.
- `/etc/alertmanager/alertmanager.yml` is the path inside the container where the Alertmanager configuration file is mounted (from your local filesystem). This file contains the routing and notification settings for Alertmanager.
- By including this line, you're ensuring that the Alertmanager container uses your custom configuration file (e.g., with Slack integration, alert routing rules, etc.) instead of its default settings.

### Why It's Important:
Without this line, Alertmanager would either:
1. Use its default configuration file (if any) or
2. Fail to start properly if no default config is available.

By specifying the `--config.file` flag, you're making sure Alertmanager uses the correct configuration file for your setup.

### Example:
- If you want to use a Slack Webhook for alert notifications, you'll specify the Webhook URL and channel in your custom `alertmanager.yml`. The `command` line ensures that Alertmanager uses this configuration when it starts.

Your `prometheus.yml` configuration file looks good for setting up basic Prometheus monitoring and alerting. Hereâ€™s a detailed explanation of each section in the configuration:

### `prometheus.yml` â€“ Configure Prometheus
```yaml
global:
  scrape_interval: 15s  # Set the global scrape interval to 15 seconds

scrape_configs:
  - job_name: "node_exporter"  # Define a job to scrape node_exporter metrics
    static_configs:
      - targets: ["node_exporter:9100"]  # Specify the target for node_exporter metrics

alerting:
  alertmanagers:
    - static_configs:
        - targets: ["alertmanager:9093"]  # Define the target for Alertmanager (where alerts are sent)

rule_files:
  - alerts.yml  # Specify the file containing alerting rules
```

```yaml
global:
  scrape_interval: 15s  # Set the global scrape interval to 15 seconds
```
#### **global**
- **scrape_interval**: This setting defines how often Prometheus will scrape metrics from the configured targets (e.g., Node Exporter, other services). In this case, Prometheus will scrape metrics every **15 seconds**.

---

```yaml
scrape_configs:
  - job_name: "node_exporter"  # Define a job named 'node_exporter'
    static_configs:
      - targets: ["node_exporter:9100"]  # Scrape metrics from Node Exporter running on port 9100
```
#### **scrape_configs**
- **job_name**: This section defines a set of targets Prometheus will scrape for metrics. In this case, the job is named `node_exporter`, which is the service you're monitoring using Node Exporter.
- **static_configs**: This defines a list of static targets (i.e., predefined targets to scrape). In your case, the target is `node_exporter:9100`, which points to the Node Exporter container that exposes system metrics on port 9100. Prometheus will scrape this target for metrics.
  
> **Note**: If you add more services to monitor (e.g., other Docker containers, services, or exporters), you can extend this section with additional jobs or targets.

---

```yaml
alerting:
  alertmanagers:
    - static_configs:
        - targets: ["alertmanager:9093"]  # Alertmanager service is running on port 9093
```
#### **alerting**
- This section defines the Alertmanager instances that Prometheus will send alerts to. 
- **alertmanagers**: This is an array of Alertmanager configurations. 
- **targets**: Here, you specify the Alertmanager service's address (`alertmanager:9093`) to which Prometheus should send alerts. In your case, it is running on port 9093.

> This allows Prometheus to notify Alertmanager whenever an alert is triggered (e.g., if a metric exceeds a certain threshold).

---

```yaml
rule_files:
  - alerts.yml  # Load custom alerting rules from alerts.yml
```
#### **rule_files**
- This section tells Prometheus to load alerting rules from the specified file.
- **alerts.yml**: This is where you define specific alerting rules (e.g., CPU usage thresholds, disk space usage, etc.). Prometheus will evaluate these rules periodically (according to `scrape_interval`) and trigger alerts if the conditions are met.

> The `alerts.yml` file should contain the actual conditions that generate alerts based on the scraped metrics.

---

### Summary of `prometheus.yml`
- **global**: Defines the scrape interval for Prometheus.
- **scrape_configs**: Defines the targets from which Prometheus scrapes metrics (in this case, `node_exporter`).
- **alerting**: Specifies the Alertmanager service that will handle alerts.
- **rule_files**: Points to the file that contains Prometheus alerting rules.


---

### ðŸ›‘ Prometheus Alert Rule Example (alerts.yml)

```yaml
groups:
  - name: node_exporter_alerts  # Define a group of alerting rules
    rules:
      - alert: HighCPUUsage  # Alert name
        expr: rate(node_cpu_seconds_total{mode="user"}[1m]) > 0.5  # PromQL expression to evaluate
        for: 1m  # Condition must hold true for 1 minute before triggering
        labels:
          severity: critical  # Add a severity label to the alert
        annotations:
          summary: "High CPU usage detected"  # Short summary of the alert
          description: "CPU usage on {{ $labels.instance }} is above the threshold."  # Detailed description
```

---

### Breakdown:

- **`groups`**:
  - The alerting rules are grouped under `node_exporter_alerts`. This helps organize the rules when there are multiple sets of alert rules.

- **`alert`**:
  - The name of the alert being triggered. In this case, it's `HighCPUUsage`.

- **`expr`**:
  - The **PromQL expression** that checks the condition for the alert. Here, it monitors the `node_cpu_seconds_total` metric to calculate the **rate** of CPU usage in the "user" mode (the percentage of CPU usage by user processes) over the past 1 minute (`[1m]`). If this rate exceeds `0.5`, the alert will be triggered.

- **`for`**:
  - The condition (`expr`) must hold for **1 minute** before the alert is triggered, preventing false positives from brief spikes.

- **`labels`**:
  - Labels are key-value pairs added to the alert. In this case, the `severity` label is set to `critical`, which can help categorize the importance of the alert.

- **`annotations`**:
  - **Summary**: A brief summary that describes the alert, visible in the Prometheus interface or Alertmanager.
  - **Description**: More detailed information, which can include dynamic fields (e.g., `{{ $labels.instance }}`) to display specific values like the affected instance.

---

### Next Steps:
1. **Test the Rule**: Once applied, this rule will trigger an alert if CPU usage exceeds 50% for 1 minute.
2. **Alertmanager Integration**: Ensure that this rule is connected to **Alertmanager** (as defined in your `prometheus.yml`) to handle notifications like emails or Slack messages.


---

## Push Gateway

### Purpose

- For ephemeral jobs or short-lived batch processes

### How It Works

1. Metrics pushed via HTTP to Pushgateway
2. Stored temporarily
3. Scraped by Prometheus

### `docker-compose.yml` Example

```yaml
version: "3.8"
services:
  pushgateway:
    image: prom/pushgateway:latest
    ports:
      - "9091:9091"
    volumes:
      - ./pushgateway-data:/data
    command:
      - "--persistence.file=/data/pushgateway.db"
```

### Example Metrics Push

```bash
echo "db_backup_duration_seconds 12.4" | \
  curl --data-binary @- http://localhost:9091/metrics/job/db-backup
```

---

## Summary

This document provides a comprehensive overview of advanced monitoring techniques using Prometheus, Alertmanager, Slack integrations, and Pushgateway for short-lived jobs. It includes storage options, alert configurations, best practices, and sample configurations for setting up a monitoring stack using Docker Compose.

