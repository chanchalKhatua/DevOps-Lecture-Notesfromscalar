# Advanced Monitoring Techniques with Prometheus

## Agenda

- Prometheus Storage
- Alertmanager
- Configuring Alerts using Slack
- Push Gateway

---

## Prometheus Storage

### Types of Storage

- **Local Storage (TSDB)**

---

## ðŸ“Š 1. **Prometheus Metric Sample**
```text
http_requests_total{method="GET", status="200"} => 250 at 2025-04-17T12:00:00Z
```

### ðŸ” Explanation:
- **`http_requests_total`**: A **counter** metric that increases over time, tracking total HTTP requests.
- **Labels**: Key-value pairs used to differentiate this metric:
  - `method="GET"` â†’ the HTTP method.
  - `status="200"` â†’ the HTTP response status code.
- **Value**: `250` means Prometheus has recorded 250 successful GET requests so far.
- **Timestamp**: When this value was recorded â†’ `2025-04-17T12:00:00Z`.

This is a basic but powerful concept: **metrics + labels** let you filter and aggregate data for dashboards, alerts, and capacity planning.

---

## ðŸ—ï¸ 2. **Prometheus Storage System**

Prometheus stores time series data locally using a custom **TSDB (Time Series Database)**.

### ðŸ—‚ï¸ Directory Structure Breakdown:

```
/prometheus
â”œâ”€â”€ WAL/                   # Write-Ahead Log
â”œâ”€â”€ chunks_head/           # Head Block (in-memory data not compacted yet)
â”œâ”€â”€ 01FJ.../               # Compacted block (2-hour segments)
â”‚   â”œâ”€â”€ chunks/            # Compressed chunks of data
â”‚   â”œâ”€â”€ index              # Label index for querying
â”‚   â””â”€â”€ meta.json          # Metadata (block start/end time)
â””â”€â”€ lock                   # Ensures only one Prometheus process accesses the data
```

### ðŸ§  Components Explained:

#### âœ… WAL/ (Write-Ahead Log)
- Temporary storage for new data **before** itâ€™s saved into blocks.
- Ensures **data recovery** in case of crashes.
- Rolled into the head block periodically.

#### âœ… chunks_head/
- **Recent in-memory data**.
- Represents **active time series**.
- Itâ€™s not yet compacted; compacted blocks only happen every **2 hours**.
- **If Prometheus is restarted, WAL + this gets replayed to restore state**.

#### âœ… Compacted Block Directory (e.g., `01FJ...`)
- Created after **compaction** (typically 2 hours of data).
- Each contains:
  - `chunks/` â†’ actual compressed samples
  - `index` â†’ fast lookup for labels & series
  - `meta.json` â†’ start and end timestamp of this block, and metadata

---

## â±ï¸ 3. **Retention Settings**

Set using Prometheus **startup flags** (not in `prometheus.yml`):

```yaml
command:
  - "--storage.tsdb.retention.time=45d"
  - "--storage.tsdb.retention.size=30GB"
```

### ðŸ’¡ Details:

#### `--storage.tsdb.retention.time=45d`
- Keep time series data for the **last 45 days**.
- Older data is **deleted automatically** to save space.

#### `--storage.tsdb.retention.size=30GB`
- Prometheus deletes **oldest blocks** if disk usage exceeds 30GB.
- Useful to prevent disk full issues.

You can set **both together**: Prometheus deletes data **when either** limit is reached.

---

## ðŸ³ 4. **Docker Compose Setup**

```yaml
prometheus:
  image: prom/prometheus:latest
  container_name: prometheus
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus
  ports:
    - "9090:9090"
  networks:
    - monitoring
  command:
    - "--config.file=/etc/prometheus/prometheus.yml"
    - "--storage.tsdb.retention.time=45d"
    - "--storage.tsdb.retention.size=30GB"
```

### ðŸ§¾ Details:

- **Volumes**:
  - `./prometheus.yml:/etc/prometheus/prometheus.yml`: Custom scrape config.
  - `prometheus_data:/prometheus`: Persist metrics between container restarts.

- **Ports**: 9090 is the Prometheus web UI.

- **Networks**: Use `monitoring` network for integration with Grafana, exporters, etc.

- **Command**: Overrides default settings to include:
  - Config file
  - Retention time (45 days)
  - Retention size (30GB)

---

## ðŸ§  5. **What prometheus.yml Is For**

The `prometheus.yml` config file is used **only for scrape-related settings**:

### ðŸŽ¯ Responsibilities:

- Scrape targets (e.g., Node Exporter, cAdvisor)
- Scrape interval (default: 15s)
- Job labels
- Relabeling rules
- External service discovery (Kubernetes, Consul, etc.)
- Recording & alerting rules
- Remote write / read

### âŒ What it does *not* include:
- **Storage configuration**
- **Retention settings**
- **Disk usage limits**

Those are **command-line flags** or Docker args, not YAML configs.

---

## ðŸ”š Summary Table

| Concept                      | Purpose                                                                 |
|-----------------------------|-------------------------------------------------------------------------|
| `http_requests_total`       | Counter metric with labels for method/status                           |
| `WAL/`                      | Stores recent data before compaction (for crash recovery)              |
| `chunks_head/`              | In-memory data not yet in blocks                                        |
| `01FJ.../` blocks            | Compacted, queryable data chunks                                       |
| `prometheus.yml`            | Configures scraping, targets, and alerts                                |
| `--storage.tsdb.retention.*`| Controls how long & how much data is kept                              |
| Docker `volumes:`           | Persists config & data on disk                                         |

---
---

# ðŸ§  Prometheus Write-Ahead Logs (WAL) and Chunks Head: In-Depth

---

## ðŸŸ¡ 1. What is a Write-Ahead Log (WAL)?

A **Write-Ahead Log** is a file where changes are **written first** before they are applied to the actual database.
1. **Buffer incoming data** in memory.
2. **Persist data on disk quickly**, before itâ€™s compacted and stored in long-term blocks.
3. **Recover data** in case Prometheus crashes or is shut down ungracefully
   
- **Purpose:** Ensures **durability** and **recovery** in case Prometheus crashes or restarts.
- **What it stores:** Every time a new sample is ingested, Prometheus **immediately writes it to the WAL** before doing
anything else.
- **Stored files:** Named sequentially (e.g. `00000014`, `00000015`, etc.), plus optional checkpoints

### ðŸ” Key Points:

| Feature | Explanation |
|--------|-------------|
| **Definition** | A file that stores incoming metric data **before** it is processed and saved permanently. |
| **Location** | `/prometheus/wal/` |
| **File Names** | Sequential: `00000001`, `00000002`, ..., plus optional `checkpoint.XXXX` folders |
| **Purpose** | Acts as a **buffered write log** for reliability and recovery |

---

## ðŸ” 2. Why WAL is Important

| Goal | How WAL Helps |
|------|---------------|
| **Data durability** | Prevents data loss during unexpected crashes |
| **Crash recovery** | Replays logs on restart to rebuild in-memory chunks |
| **High performance** | Buffers writes efficiently before compaction |

---

## ðŸ§­ 3. WAL Workflow â€“ Step-by-Step

### ðŸ”‚ Data Flow:
1. ðŸŸ¢ Scrape: Prometheus scrapes metrics from targets.
2. ðŸ“ **Write to WAL**: The sample is immediately written to a WAL segment.
3. ðŸ§  **Store in memory (Chunks Head)** for fast querying.
4. ðŸ“¦ Every 2 hours, data is **compacted into TSDB blocks**.
5. ðŸ§¹ Old WAL segments are deleted after checkpointing.
---
![image](https://github.com/user-attachments/assets/0b8f6eb0-ea30-4adc-858c-bd6452d40067)

---

## ðŸ“„ 4. WAL File Types

| Type | Description |
|------|-------------|
| `00000001`, `00000002`, etc. | Actual WAL segment files with ingested samples |
| `checkpoint.XXXX` folders | Snapshots used to **speed up recovery** |
| Metadata files | Track segment start/end positions, transaction markers |

---

## ðŸ§  5. Chunks Head â€” Active In-Memory Buffer

**Chunks Head** is where Prometheus stores **active time series data** before it's flushed into a permanent block.

- **Purpose:** This holds the actual **in-memory time series chunks** before they are flushed into persistent block
storage (~2-hour blocks).
- **What it stores:** Time series data points that are **actively being collected and not yet compacted**.
- **When itâ€™s used:** These chunks are **not flushed to disk** until a block is created. Until then, they live in memory and
are backed by WAL.

### ðŸ§¾ Details:

| Attribute | Description |
|-----------|-------------|
| **Path** | `/prometheus/chunks_head/` |
| **Purpose** | Holds in-memory chunks for fast updates & reads |
| **Backed by** | WAL (for durability) |
| **Compacted every** | ~2 hours (default) |
| **Stored data** | Raw time series samples (`timestamp`, `value`, `labels`) |

---

## ðŸ” 6. Relationship Between WAL & Chunks Head

| Step | WAL | Chunks Head |
|------|-----|--------------|
| Initial write | âœ… (on disk) | âœ… (in memory) |
| During active collection | Used for recovery | Used for fast querying |
| After compaction | May be checkpointed & deleted | Flushed into block |
| On restart | WAL is replayed | Chunks Head is reconstructed |

---

## ðŸ§± 7. TSDB Block Structure (Created from Chunks Head)

Every 2 hours, data in Chunks Head is compacted into a block directory:

```
/prometheus/01FJXXXX/
â”œâ”€â”€ chunks/      # Compressed time series data
â”œâ”€â”€ index        # Label/series index
â””â”€â”€ meta.json    # Metadata (timestamps, source info)
```

---

## ðŸ—ƒï¸ 8. WAL Lifecycle in Summary

```plaintext
Scrape â†’ WAL (disk) â†’ Chunks Head (memory) â†’ TSDB Block (permanent)
```

1. **Scraped** sample is written to WAL and memory.
2. WAL persists data even if Prometheus crashes.
3. Chunks Head builds compressed chunks.
4. Compactor creates a TSDB block every 2h.
5. WAL segments are **checkpointed and deleted** post-block creation.

---

## ðŸ”§ 9. How to Configure Retention (Optional but Important)

These are not in `prometheus.yml`, but passed as flags in Docker or systemd:

```yaml
command:
  - "--storage.tsdb.retention.time=45d"
  - "--storage.tsdb.retention.size=30GB"
```

| Flag | Purpose |
|------|---------|
| `--storage.tsdb.retention.time` | Retain time series for 45 days |
| `--storage.tsdb.retention.size` | Cap storage usage at 30GB |

---

## ðŸ“Œ 10. Key Facts Summary Table

| Concept          | Description |
|------------------|-------------|
| WAL              | Fast disk write log for durability |
| Chunks Head      | In-memory buffer of recent time series |
| Checkpoint       | Optimized snapshot of WAL |
| Compaction       | Happens ~every 2h, flushes memory to disk |
| Block Directory  | Contains `chunks`, `index`, and `meta.json` |
| WAL Files        | Sequentially numbered, deleted post-compaction |
| Recovery         | WAL replayed to rebuild memory after crash |

---

### Remote Storage
---

# ðŸ“¦ Prometheus Remote Storage and Best Practices

## Remote Storage Integration
For **long-term durability** and **centralized querying**, Prometheus can **integrate with remote storage systems**.

Prometheus supports two types of remote integrations:

- **Remote Write**: Push data to external systems  
  (e.g., **Thanos**, **Cortex**, **VictoriaMetrics**, **InfluxDB**)

- **Remote Read**: Query remote data back into Prometheus UI / via PromQL.

### Example Configuration:
```yaml
remote_write:
  - url: "http://thanos-receive:10902/api/v1/receive"

remote_read:
  - url: "http://thanos-query:9090"
```

---

# âš™ï¸ Thanos Integration (for Centralized, Scalable Storage)
- **Thanos Receive** will accept remote writes.
- **Thanos Query** will serve remote reads, allowing you to query long-term and short-term data seamlessly.

---

# ðŸ–¥ï¸ Node Exporter Metrics
- **Exporter Port**: `9100`
- Used to collect **node (server) metrics** like CPU, memory, disk usage, etc.

---

# ðŸ›¡ï¸ Best Practices for Prometheus Storage

1. **Monitor Disk Space**  
   - Always monitor your Prometheus server's storage.
   - Alert if usage crosses critical thresholds.

2. **Optimize Retention Periods**  
   - Set Prometheus' `--storage.tsdb.retention.time` wisely based on your needs (e.g., 15 days, 30 days).
   
3. **Use External Storage**  
   - For long-term retention and scalability, integrate external systems like **Thanos**, **Cortex**, etc.

4. **Implement Sharding**  
   - In large-scale environments, deploy **multiple Prometheus instances**.
   - Use a load balancer or scrape sharding to distribute the ingestion load.

5. **Regular Cleanup**  
   - Configure automatic cleanup of old data.
   - Ensure blocks or WAL files older than the retention period are deleted automatically.

---
---

## Alertmanager
### What are alerts?
 - Resource exhaustion (e.g., high CPU or memory usage).
 - Failures (e.g., a service going down).
 - Threshold breaches (e.g., response time exceeding acceptable limits).
### When to trigger alerts
 1. Performance Issues:
     - High CPU or memory usage, slow database queries, or application response times out of range.
 2. Failures:
     - Service crashes, database outages, or network failures.
 3. Security Events:
     - Unauthorized access attempts or unusual traffic spikes.
 4. Business Metrics:
     - Revenue drops, user signups, or payment failures.
### Why Alerts Are Needed
1. Proactive Monitoring:
    - Detect issues before they affect users or customers.
    - Alerts help anticipate issues before they escalate into major problems, improving system reliability and user
     experience.
2. Prevent Downtime:
    - Immediate alerts allow teams to respond before small issues become major outages.
3. Compliance and SLA Management:
    - Ensure systems comply with service level agreements (SLAs) by notifying when performance drops below agreed
      levels.
 4. Automation and Speed:
    - Alerts automate the process of notifying your team about issues.
    - Without alerts, issues might go unnoticed, leading to prolonged downtime or degraded performance.
5. Quick Response to Critical Issues:
    - Alerts provide real-time visibility into problems, allowing teams to respond quickly and prevent cascading
      failures.
6. Avoid Alert Fatigue:
    - Properly configured alerts highlight important issues while filtering out noise.
7. Improve Decision-Making:
    - Alerts provide insights that can inform decisions, such as prioritizing resources to address a high number of failed
      requests or a network bottleneck.
### Best Practices for alerting
 1. Set Appropriate Thresholds:
   - Avoid setting thresholds too low or too high.
 2. Avoid Alert Fatigue:
   - Configure alerts thoughtfully to reduce noise.
 3. Use Multiple Notification Channels:
   - Ensure alerts reach the right people through email, Slack, SMS, etc.
 4. Regularly Review Alerting Policies:
   - Periodically review and update alerting rules to ensure they remain relevant.
### Alert Types

- Threshold Alerts
- State Change Alerts
- Anomaly Alerts
- Business Metrics

### Features

1. Ingestion: Receives alerts from Prometheus or other clients.
2. Deduplication: Prevents duplicate notifications for the same issue.
3. Grouping: Groups related alerts together to avoid overwhelming your team.
4. Inhibition: Suppresses alerts based on predefined rules (e.g., suppressing low-priority alerts when a high-priority alert
is active).
5. Silencing: Temporarily suppresses alerts during maintenance or known issues.
6. Routing: Sends alerts to different receivers (e.g., email, Slack) based on rules.

In summary Alertmanager is responsible for
- **Receiving alerts** from Prometheus
- **Deduplicating** them
- **Grouping** related alerts
- **Routing** them to the correct notification channels (like Email, Slack, PagerDuty)
- **Silencing** or **inhibiting** alerts when needed (to avoid alert fatigue)

### Workflow

1. Prometheus triggers alert
2. Alertmanager applies rules
3. Alertmanager sends notification
### How does Alert manager work??
1. Prometheus Triggers an Alert:
 - Based on a condition, such as CPU Usage > 90%.
2. Alertmanager Processes the Alert:
 - It applies rules for grouping, silencing, or inhibition.
3. Alertmanager Sends Notifications:
 - Alerts are routed to the appropriate receivers, such as email, Slack, or PagerDuty.

 1. Alerting Rules in Prometheus: Define when alerts should be triggered.
 2. Alertmanager Configuration: Defines how alerts are handled and routed.
### Alert Rule Example (`alerts.yml`)

```yaml
groups:
  - name: example-alerts
    rules:
      - alert: HighCPUUsage
        expr: avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) > 0.85
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 85% for 2 minutes on {{ $labels.instance }}"

```

### Configure Prometheus to Use Alert Rules

```yaml
global:
  scrape_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']

scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']

```

### Alertmanager Email Example

Awesome â€” you've shared a basic **Alertmanager email notification configuration**!  
Let me quickly organize and slightly format it so itâ€™s clean and easy to use:

---

```yaml
global:
  resolve_timeout: 5m

route:
  receiver: 'email-team'

receivers:
  - name: 'email-team'
    email_configs:
      - to: 'devops-team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_identity: 'alertmanager@example.com'
        auth_password: 'your_smtp_password'
```

---

# âœ¨ Key Points:
- **global.resolve_timeout**:  
  If an alert resolves, Alertmanager will wait 5 minutes before considering it resolved (allows for flapping alerts).
  
- **route.receiver**:  
  The default receiver for alerts is `email-team`.

- **receivers.email_configs**:
  - **to**: Where the alert emails are sent (`devops-team@example.com`).
  - **from**: The sender email address.
  - **smarthost**: SMTP server with port (typically 587 for TLS).
  - **auth_username / auth_identity / auth_password**: SMTP authentication credentials.

---
### **"End-to-End Flow"** 
---

ðŸ”¹ **Node Exporter**  
â†’ Exposes system metrics (CPU, memory, disk, etc.)

ðŸ”¹ **Prometheus**  
â†’ Pulls/scrapes metrics from Node Exporter  
â†’ Evaluates alert rules in `alert_rules.yml`

ðŸ”¹ **If alert triggered**  
â†’ Prometheus fires the alert  
â†’ Sends it to **Alertmanager**

ðŸ”¹ **Alertmanager**  
â†’ Manages alerts (grouping, silencing, routing)  
â†’ Sends notifications via Email, Slack, PagerDuty, etc.

---
flowchart TD
    A[Node Exporter] --> B[Prometheus Scrapes Metrics]
    B --> C{Evaluate Alert Rules}
    C -- No Alert --> B
    C -- Alert Triggered --> D[Send Alert to Alertmanager]
    D --> E{Route & Manage Alerts}
    E --> F[Send Notification (Email / Slack / etc.)]


---

## Configuring Alerts Using Slack

### Steps

1. Create Slack channel
2. Create Incoming Webhook from Slack API
3. Configure `alertmanager.yml` with webhook

```yaml
route:
  receiver: slack
receivers:
  - name: slack
    slack_configs:
      - channel: "#temp-devops-assignment-1"
        api_url: "https://hooks.slack.com/services/..."
        send_resolved: true
        text: |
          *Alert:* {{ .CommonAnnotations.summary }}
          {{ .CommonAnnotations.description }}
```

---

## Push Gateway

### Purpose

- For ephemeral jobs or short-lived batch processes

### How It Works

1. Metrics pushed via HTTP to Pushgateway
2. Stored temporarily
3. Scraped by Prometheus

### `docker-compose.yml` Example

```yaml
version: "3.8"
services:
  pushgateway:
    image: prom/pushgateway:latest
    ports:
      - "9091:9091"
    volumes:
      - ./pushgateway-data:/data
    command:
      - "--persistence.file=/data/pushgateway.db"
```

### Example Metrics Push

```bash
echo "db_backup_duration_seconds 12.4" | \
  curl --data-binary @- http://localhost:9091/metrics/job/db-backup
```

---

## Summary

This document provides a comprehensive overview of advanced monitoring techniques using Prometheus, Alertmanager, Slack integrations, and Pushgateway for short-lived jobs. It includes storage options, alert configurations, best practices, and sample configurations for setting up a monitoring stack using Docker Compose.

