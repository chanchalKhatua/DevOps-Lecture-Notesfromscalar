# Advanced Monitoring Techniques with Prometheus

## Agenda

- Prometheus Storage
- Alertmanager
- Configuring Alerts using Slack
- Push Gateway

---

## Prometheus Storage

### Types of Storage

- **Local Storage (TSDB)**
  - `chunks_head/`: in-memory time series data (head block)
  - Compacted blocks (2-hour chunks):
    - `chunks/`: compressed time series
    - `index`: label/series index
    - `meta.json`: block metadata

### Retention

- Time-based: `--storage.tsdb.retention.time=45d`
- Size-based: `--storage.tsdb.retention.size=30GB`

### WAL (Write-Ahead Logs)

- Buffers incoming data
- Persists to disk before compaction
- Used for crash recovery
- Files: `00000014`, checkpoints, etc.

### Remote Storage

- **Remote Write**: Thanos, Cortex, VictoriaMetrics, InfluxDB
- **Remote Read**: query external systems

```yaml
remote_write:
  - url: "http://thanos-receive:10902/api/v1/receive"
remote_read:
  - url: "http://thanos-query:9090"
```

### Best Practices

1. Monitor Disk Space
2. Optimize Retention
3. Use External Storage
4. Implement Sharding
5. Regular Cleanup

---

## Alertmanager

### Why Alerts Are Needed

- Proactive Monitoring
- Prevent Downtime
- SLA Compliance
- Automation
- Faster Response
- Reduce Alert Fatigue
- Better Decision-Making

### Alert Types

- Threshold Alerts
- State Change Alerts
- Anomaly Alerts
- Business Metrics

### Features

- Ingestion, Deduplication, Grouping
- Inhibition, Silencing, Routing

### Workflow

1. Prometheus triggers alert
2. Alertmanager applies rules
3. Alertmanager sends notification

### Alert Rule Example (`alerts.yml`)

```yaml
groups:
  - name: node_exporter_alerts
    rules:
      - alert: HighCPUUsage
        expr: rate(node_cpu_seconds_total{mode="user"}[1m]) > 0.5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage on {{ $labels.instance }} is above the threshold."
```

### Configure Prometheus to Use Alert Rules

```yaml
global:
  scrape_interval: 15s
rule_files:
  - alerts.yml
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']
```

### Alertmanager Email Example

```yaml
route:
  receiver: 'email-team'
receivers:
  - name: 'email-team'
    email_configs:
      - to: 'devops-team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'your_smtp_password'
```

---

## Configuring Alerts Using Slack

### Steps

1. Create Slack channel
2. Create Incoming Webhook from Slack API
3. Configure `alertmanager.yml` with webhook

```yaml
route:
  receiver: slack
receivers:
  - name: slack
    slack_configs:
      - channel: "#temp-devops-assignment-1"
        api_url: "https://hooks.slack.com/services/..."
        send_resolved: true
        text: |
          *Alert:* {{ .CommonAnnotations.summary }}
          {{ .CommonAnnotations.description }}
```

---

## Push Gateway

### Purpose

- For ephemeral jobs or short-lived batch processes

### How It Works

1. Metrics pushed via HTTP to Pushgateway
2. Stored temporarily
3. Scraped by Prometheus

### `docker-compose.yml` Example

```yaml
version: "3.8"
services:
  pushgateway:
    image: prom/pushgateway:latest
    ports:
      - "9091:9091"
    volumes:
      - ./pushgateway-data:/data
    command:
      - "--persistence.file=/data/pushgateway.db"
```

### Example Metrics Push

```bash
echo "db_backup_duration_seconds 12.4" | \
  curl --data-binary @- http://localhost:9091/metrics/job/db-backup
```

---

## Summary

This document provides a comprehensive overview of advanced monitoring techniques using Prometheus, Alertmanager, Slack integrations, and Pushgateway for short-lived jobs. It includes storage options, alert configurations, best practices, and sample configurations for setting up a monitoring stack using Docker Compose.

